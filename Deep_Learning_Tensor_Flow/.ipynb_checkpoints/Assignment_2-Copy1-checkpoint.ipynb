{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hsing-Yi Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the CIFAR data set reader from the first homework and read the the CIFAR-10 files again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import functools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    dict = unpickle(file)\n",
    "    data = dict[b'data']\n",
    "    labels = np.asarray(dict[b'labels'], dtype=np.int64)\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbatch(alldata, alllabels, batch_size=16):\n",
    "    nrows_data = int(len(alldata)/batch_size)\n",
    "    nrows_labels = int(len(alllabels)/batch_size)\n",
    "    # create new array\n",
    "    re_data = alldata.reshape(-1, batch_size, int( 3 * 32 * 32))\n",
    "    re_labels = alllabels.reshape(-1, batch_size, 10)\n",
    "    \n",
    "    return (re_data, re_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(path=''):\n",
    "    classes = [s.decode('UTF-8') for s in unpickle(path+'/Users/eve7947/Downloads/cifar-10-batches-py/batches.meta')[b'label_names']]\n",
    "    X_train, trainLabels = [], []\n",
    "    for i in range(5):\n",
    "        data, labels = read_data(path+'/Users/eve7947/Downloads/cifar-10-batches-py/data_batch_%d'%(i+1))\n",
    "        X_train.append(data)\n",
    "        trainLabels.append(labels)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    trainLabels = np.concatenate(trainLabels)\n",
    "    \n",
    "    testData, testLabels = read_data(path+'/Users/eve7947/Downloads/cifar-10-batches-py/test_batch')\n",
    "    return classes, X_train, trainLabels, testData, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, X_train, trainLabels, testData, testLabels = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "X_train_shape (50000, 3072)\n",
      "trainLabels [6 9 9 ... 9 1 1]\n",
      "trainLabels (50000,)\n",
      "testData_shape (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print('classes',classes)\n",
    "print('X_train_shape',X_train.shape)\n",
    "print('trainLabels',trainLabels)\n",
    "print('trainLabels',trainLabels.shape)\n",
    "print('testData_shape',testData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply random noise to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check data[] -> 0\n",
      "[38357 20352 38292 ... 24642 41123 18703]\n"
     ]
    }
   ],
   "source": [
    "row_noise = np.random.randint(0, len(X_train), int(len(X_train)*len(X_train[0])*0.1))\n",
    "col_noise = np.random.randint(0, len(X_train[0]), int(len(X_train)*len(X_train[0])*0.1))\n",
    "X_train[(row_noise, col_noise)] = 0\n",
    "\n",
    "print('check data[] -> {}'.format(X_train[row_noise[0], col_noise[0]]))\n",
    "print(row_noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train -> [[ 59  43  50 ... 140  84  72]\n",
      " [154 126 105 ... 139   0   0]\n",
      " [255   0 253 ...  83  83   0]\n",
      " ...\n",
      " [ 35  40  42 ...  77   0  50]\n",
      " [189 186 185 ... 169   0 171]\n",
      " [229 236 234 ... 173 162   0]]\n",
      "[[0.23137255 0.16862746 0.19607843 ... 0.54901963 0.32941177 0.28235295]\n",
      " [0.6039216  0.49411765 0.4117647  ... 0.54509807 0.         0.        ]\n",
      " [1.         0.         0.99215686 ... 0.3254902  0.3254902  0.        ]\n",
      " ...\n",
      " [0.13725491 0.15686275 0.16470589 ... 0.3019608  0.         0.19607843]\n",
      " [0.7411765  0.7294118  0.7254902  ... 0.6627451  0.         0.67058825]\n",
      " [0.8980392  0.9254902  0.91764706 ... 0.6784314  0.63529414 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('X_train -> {}'.format(X_train))\n",
    "data1 = np.array(X_train/255, dtype=np.float32)\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert all labels to onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "onehot_labels = np.full((len(trainLabels), 10), 0)\n",
    "onehot_labels[np.arange(0, len(trainLabels)), trainLabels] = 1\n",
    "print(onehot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000, 3072)\n",
      "(10, 5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# reshape data and labels by batch size data_x(3125, 49152), data_y(3125, 160) \n",
    "batch_size = 5000\n",
    "X_train_x, trainLabels_y = getbatch(data1, onehot_labels, batch_size)\n",
    "print(X_train_x.shape)\n",
    "print(trainLabels_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "### 6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "### 7. Also write the cost / loss at the end of each epoch to tensorboard.\n",
    "### 8. Train the network with learning rates of [0.1, 0.01, 0.001]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.01\n",
    "#learning_rate = 0.1\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 5000\n",
    "display_step = 1\n",
    "n_sample = X_train.shape[0]\n",
    "noofbatches = n_sample//batch_size\n",
    "\n",
    "ninput = 3072\n",
    "nhidden1 = 512\n",
    "nhidden2 = 256\n",
    "nhidden3 = 128\n",
    "n_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [None, ninput])\n",
    "Y = tf.placeholder('float', [None, n_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/eve7947/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "weight = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([ninput, nhidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([nhidden1, nhidden2])),\n",
    "        'h3': tf.Variable(tf.random_normal([nhidden2, nhidden3])), \n",
    "        'out': tf.Variable(tf.random_normal([nhidden3, n_class]))\n",
    "}\n",
    "\n",
    "bias = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([nhidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([nhidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([nhidden3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_class]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiperceptron(X):\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(X, weight['h1']), bias['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weight['h2']), bias['b2']))\n",
    "    l3 = tf.nn.sigmoid(tf.add(tf.matmul(l2, weight['h3']), bias['b3']))\n",
    "    outl = tf.nn.sigmoid(tf.add(tf.matmul(l3, weight['out']), bias['out']))\n",
    "    return outl\n",
    "    \n",
    "model = multiperceptron(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-10ebd5b7fd18>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs\", graph=tf.get_default_graph())\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# write weight\n",
    "tf.summary.histogram(\"w1\", weight['h1'])\n",
    "tf.summary.histogram(\"w2\", weight['h2'])\n",
    "tf.summary.histogram(\"w3\", weight['h3'])\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.3180, Accuracy: 0.0950\n",
      "Epoch: 1, Loss: 2.2823, Accuracy: 0.0950\n",
      "Epoch: 2, Loss: 2.2537, Accuracy: 0.0950\n",
      "Epoch: 3, Loss: 2.2342, Accuracy: 0.0950\n",
      "Epoch: 4, Loss: 2.2209, Accuracy: 0.0950\n",
      "Epoch: 5, Loss: 2.1998, Accuracy: 0.0950\n",
      "Epoch: 6, Loss: 2.1651, Accuracy: 0.0950\n",
      "Epoch: 7, Loss: 2.1442, Accuracy: 0.0950\n",
      "Epoch: 8, Loss: 2.1319, Accuracy: 0.0950\n",
      "Epoch: 9, Loss: 2.1184, Accuracy: 0.0950\n",
      "Testing accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        for i in range(noofbatches):\n",
    "            batch_x = X_train_x[i]\n",
    "            batch_y = trainLabels_y[i]\n",
    "            # Use training data for optimization\n",
    "            _, merged_summary = sess.run([train_min,merged_summary_op], feed_dict={X:batch_x, Y:batch_y})\n",
    "            # sess.run(train_min, feed_dict={X:batch_x, Y:batch_y})\n",
    "            \n",
    "        writer.add_summary(merged_summary, epoch)\n",
    "        \n",
    "        # Validate after every epoch\n",
    "        batch_x = X_train_x[i]\n",
    "        batch_y = trainLabels_y[i]\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:batch_x, Y:batch_y})\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "            \n",
    "    # When the training is complete and you are happy with the result\n",
    "    accuracycalc = sess.run(accuracy, feed_dict={X: data1, Y: onehot_labels})\n",
    "    print(\"Testing accuracy: %0.4f\"%(accuracycalc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
